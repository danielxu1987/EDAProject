---
title: "EDA Project"
output:
  html_document: default
---

This is the R Notebook for the project from Sifeng Xu, 24525844 for unit *CITS4009 Computation Data Analysis*. It demonstrates the process and findings of EDA based on the **Countries and Death Causes** dataset. 

```{r setup, echo=FALSE}
# Load  libraries
# library(shiny)
# library(shinyWidgets)
library(ggplot2)
library(gridExtra)
library(knitr)
library(dplyr)


# load dataset
df_original <- read.csv("./Countries and death causes.csv",header = T, sep=",")

df <- df_original
years <- 1990:2019
dietVars <- c('Diet.low.in.whole.grains', 'Diet.low.in.fruits', 'Diet.low.in.Vegetables', 'Diet.low.in.nuts.and.seeds', 'Diet.high.in.sodium')

dietTargetVar <- c('Diet.high.in.sodium')
dietFeatureVars <- dietVars[dietVars != dietTargetVar]

count_nas <- function(data){
  count_missing <- function(data){
    sapply(data, FUN=function(col) sum(is.na(col)))
  }
  nacounts <- count_missing(data)
  hasNA = which(nacounts > 0)
  nacounts[hasNA]
}

count_null <- function(data){
  count_missing <- function(data){
    sapply(data, FUN=function(col) sum(col == ''))
  }
  nullcounts <- count_missing(data)
  hasNull = which(nullcounts > 0)
  nullcounts[hasNull]
}

count_negative <- function(data){
  count_missing <- function(data){
    sapply(data, FUN=function(col) sum(col < 0))
  }
  negcounts <- count_missing(data)
  hasNegative = which(negcounts > 0)
  negcounts[hasNegative]
}

dataUK <- df[df['Entity']=='United Kingdom',] #df[df['Entity']=='China',]
dataTAN <- df[df['Entity']=='Tanzania',]
dataNLD <- df[df['Entity']=='Netherlands',]
dataCAM <- df[df['Entity']=='Cambodia',]

```

### Basic information of the data


```{r glance, echo=TRUE}

data.frame(
  columeID = col(df)[1,],
  variables = names(df),
  class = sapply(df, typeof),
  first_values = sapply(df, function(x) paste0(head(x), collapse = ', ')),
  row.names = NULL) |>
kable()

summary(df) |> kable()

head(df) |> kable()

```


A first glance of data shows 31 columns which consists of 3 non-numerical data:

* countries' full name
* short country code
* year of measurements (1990-2019) 

the rest is numerical data showing how many deaths from each cause, each year and each country/region. Some countries are counted together such as *G20, OECD Countries, South-East Asia Region* and *Western Passific Region*. Also, some measured variables look similar such as *Outdoor.air.pollution, Household.air.pollution.from.solid.fuels* and *Air.pollution*.  


To give an impression what the dataset looks like, a snippet shows 6 countries and their deaths count from several causes in 2010.

```{r snippet, include=TRUE}
df_2010 <- df[df['Year']==2010, 
  c('Entity', 'Code', 'Outdoor.air.pollution','Child.wasting', 'Smoking', 'Drug.use')]
countries <- c('Afghanistan', 'Morocco', 'Nigeria', 'Australia', 'Belgium', 'Congo')
snippet <- subset(df_2010, Entity %in% countries)
names(snippet)[1] <- 'Country'
print(snippet)
```


### Histogram for single death cause

First check the data range by histogram. Variables that are linked to environment such as outdoor and household air quality tend to cause more deaths. This may prove that the long-term influence of poor air quality, which often sustains through a longer period, can kill many people living in that area/region.

![Outdoor air pollution](./images/hist_single_oap.JPG)
![Household air pollution from solid fuels](./images/hist_single_hap.JPG)

In contrast, causes such as *iron deficiency* and *vitamin A deficiency* lead to far less mortalities worldwide. Probably lack of iron or vitamin A alone is not too critical to the survival of most people.

![Iron deficiency](./images/hist_single_iron.JPG)
![Vitamin A deficiency](./images/hist_single_va.JPG)

Across all variables, a fact is that high value of mortality does not appear often as most values seem to stay close to the lower boundary. Note a spike near zero for all example figures.

### Boxplot single

After comparing different countries with boxplot, it is noticeable that developed countries usually have less mortalities than developing countries in causes related to poor hygiene (e.g. water quality, unsafe sanitation and no access to hand washing facility). 

![Compare countries - unsafe water source](./images/box_water.JPG)

![Compare countries - unsafe sanitation](./images/box_sani.JPG)

![Compare countries - no access to handwashing facility](./images/box_handw.JPG)

Two pairs of countries are selected here as examples between the *Uk* and *Tanzania*, as well as the *Netherlands* and *Cambodia*. The reason why they are compared to each other is because counties in each pair shares similar population according to [Worldometers](https://www.worldometers.info/world-population/population-by-country/).

### Trend comparison

Line graph shows the trend of certain causes from 1990 to 2019. Comparison between countries shows interesting facts.  Alcohol use seems to be more detrimental to people's health in recent years in both developed and developing countries selected in previous section. 

![](./images/trend_alco.JPG)

On the other hand, outdoor air pollution appears to become less harmful in western Europe than in Africa or Asia. This may have to do with the progress of industrialisation which leads to air pollution. In other words, Europe was almost at the end of it around the turn of the century, and they paid more attention to the environmental impact. While certain areas in Africa and Asia just started to industrialize themselves. That is why the air quality got worse and worse in this period there.

![](./images/trend_oap.JPG)

### Bar chart

Drug use seems to have serious impact in both rich and poor areas. In the figure below, far more people ended their life in India and the USA due to drug use than the whole African Region in 2010. European region is not much better as they ranked the third in this set of comparison worldwide.

![](./images/Bar_compare_us.JPG)

### Correlation between variables

When comparing multiple variables, it is clear that diet related death causes are  positive linear for African region. This may indicate the food security issue in this area.  On the other hand, this correlation seems less obvious for North America where the general public have much more dietary options.

![](./images/pair_diet_afr.JPG)

![](./images/pair_diet_northa.JPG)

Smooth curve and jittering points shows drug use and unsafe sex clustering when sex fatality value is low.  Once the value grows further they seemed less correlated.

![](./images/smooth_sexdrug_lim.JPG)

### Missing values and anomolies

These values are checked in all numerical variables: *NA, null, zeros* and *negative numbers*. 

```{r nas, echo=TRUE} 

numericVars <- names(df[, -c(1:3)]) # ignore Entity, Code and Year columns
df_num <- df[numericVars]

print(sum(is.na(df_num)))

print(sum(is.null(df_num)))

print(sum(df_num < 0))

print(sum(df_num == 0))
```

Currently there is no NA's in the dataset, neither are there any negative values. There are zero values, which may indicate that either no one died of that specific cause in a country in a specific year, or they were missing values. Although it is possible to treat zero's as missing values and replace them with e.g. median or mean, doing this will overwrite all the zero's which are truly reflecting zero mortality.  Therefore, in this exploration attempt we will keep zero's as-is.

When checking the first three columns it shows that *Code* are empty in 690 rows. These appear to be specific to aggregated regions e.g. OECD Countries, or regions within a country without a valid country code e.g. Scotland.

```{r rm_code, echo=TRUE}

count_empty <- function(data){
  count_missing <- function(data){
    sapply(data, FUN=function(col) sum(col == ''))
  }
  nullcounts <- count_missing(data)
  hasNull = which(nullcounts > 0)
  nullcounts[hasNull]
}

print(count_empty(df))
empty_rows <- df[df$Code == '', ]
print(empty_rows[sample(nrow(empty_rows), size = 6, replace = F), c(1:6)])
```

Make it easier to understand the sections below we rename Entity to Country. Also, since we will merge external data into this dataset based on country code, those aggregated entities will not be receiving any data. Therefore we would like to drop them now and keep only data per country, not per group, for further study.

```{r rmcode}
colnames(df)[1] <- "Country"
df <- df[df$Code != '', ]
```


### Classfication 

We feel that the mortality count alone may not tell us much. So we would like to include GDP per capita and country's population. Now we are going to merge the dataset as provided from [World Bank](https://data.worldbank.org/indicator/NY.GDP.PCAP.CD), and check missing values immediately.

```{r merge_gdp, echo=TRUE}
library(reshape2)

# # merge gdp per capita
# gdp <- read.table('./API_NY.GDP.PCAP.CD_DS2_en_csv_v2_31681/API_NY.GDP.PCAP.CD_DS2_en_csv_v2_31681.csv',header=T, sep=',')
# colnames(gdp) <- gsub("^X", "", colnames(gdp))
# 
# gdp <- gdp[, c('Country.Code', years)]
# gdp_df <- melt(gdp, id.vars = "Country.Code", variable.name = "Year", value.name = "gdp.per.capita")
# gdp_df$Year <- as.numeric(as.character(gdp_df$Year))
# df_merged_gdp <- merge(df, gdp_df, by.x = c("Code", "Year"), by.y = c("Country.Code", "Year"), all.x = TRUE)


# merge total population 
popl <- read.table('./API_SP.POP.TOTL_DS2_en_csv_v2_31753/API_SP.POP.TOTL_DS2_en_csv_v2_31753.csv',header=T, sep=',')
colnames(popl) <- gsub("^X", "", colnames(popl))

popl <- popl[, c('Country.Code', years)]
popl_df <- melt(popl, id.vars = "Country.Code", variable.name = "Year", value.name = "total.population")
popl_df$Year <- as.numeric(as.character(popl_df$Year))
df_merged <- merge(df, popl_df, by.x = c("Code", "Year"), by.y = c("Country.Code", "Year"), all.x = TRUE)

df_merged <- df_merged[, c(1:3, ncol(df_merged)-1, ncol(df_merged), 4:(ncol(df_merged)-2))]

nacount <- count_nas(df_merged)
print(nacount)

```

It appears that some islands and the total value of the whole world from the original death causes are also included which results in `r nacount` NAs in both gdp and total population. This is only `r nacount/nrow(df_merged)*100`% of all dataset. Since we are only intereted in studying individual countries it ok to drop them.

```{r drop_nas, echo=TRUE}

nas_df_merged <- df_merged[is.na(df_merged$total.population),]
df_merged <- setdiff(df_merged, nas_df_merged)

# Our new dataset looks like:
df_merged[sample(nrow(df_merged), 6), 1:6]

```

We would like to know more about the death cause that have large impact worldwide. Since the absolute number of mortality can vary largely depending on the population of a country, a better measurement is the percentage of each death cause, with respect to the total casualties in that row. 

We will first create a rate to count the percentage of each specific casualty cause. Then visualise the stats with boxplot to see the major players worldwide. Also country and year will be converted to factor.

```{r addCat, echo=TRUE}

df_merged$Country <- as.factor(df_merged$Country)
df_merged$Year <- as.factor(df_merged$Year)

df_merged$total.mortality <- rowSums(df_merged[numericVars])

for (col in numericVars) {
    df_merged[paste0(col, ".rate")] <- df_merged[col] / df_merged['total.mortality']
}

rateVars <- grep("\\.rate$", names(df_merged), value = TRUE)

figRateAll <- ggplot(stack(df_merged[rateVars])) + geom_boxplot(mapping = aes(x = ind, y = values)) + labs(x ='', y='Weight in total mortality') + coord_flip() 

print(figRateAll)

# count the death rate of all causes with respect to the total population
df_merged$Death.rate <- (df_merged$total.mortality / df_merged$total.population)
```

### Single variable prediction
It seems high blood pressure, smoking, high glucose rate and high BMI (obesity) are the top reasons to people's death. Now we would like to see what causes high death rate to a country. So we will first generate an outcome column which indicates if death rate is higher than the worldwide median. This variable holds the ground truth to check against the prediction later. 

Our hypothesis is that the higher percentage of top death causes the more likely death rate will be high. So we will also derived binary values from *Unsafe.sex*, *High.blood.pressure* and *Smoking* to see if these death contribution is over their median values or not. Another reason to make extra binary is because so far there are only years and country which can be converted to factor. We would like to create a couple of extra categorical variables. Code is less self-explanatory than Country so we will remove it. 

Now it is time to separate the data into training (81%), calibration(9%) and test (10%) data.

```{r get_train_cal_test, echo=TRUE}

df_merged$Code <- NULL

outcome <- c('Death.rate.high') # Smoking.kills.more, sodium.over.median
pos <- TRUE # positive value 

df_merged <- df_merged |>
  mutate(
    Death.rate.high = (Death.rate > median(Death.rate)),
    Unsafe.sex.over.median = (Unsafe.sex.rate > median(Unsafe.sex.rate)),
    Smoking.over.median = (Smoking.rate > median(Smoking.rate)),
    Blood.pressure.over.median = (High.systolic.blood.pressure.rate < median(High.systolic.blood.pressure.rate))
)

d <- df_merged

set.seed(729375)
d$rgroup <- runif(dim(d)[1])
dTrainAll <- subset(d, rgroup<=0.9)
dTest <- subset(d, rgroup>0.9)

# split dTrainAll into a training set and a validation (or calibration) set
useForCal <- rbinom(n=dim(dTrainAll)[1], size=1, prob=0.1)>0
dCal <- subset(dTrainAll, useForCal)
dTrain <- subset(dTrainAll, !useForCal)
# 
# print(dim(dTrain))
# 
# print(dim(dCal))
# 
# print(dim(dTest))

```

The next step is to go through the categorical variables and see which one has the best prediction chance. Here the null model is saying the prediction will be the overall proportion of True values, regardless what input it receives. Therefore, anything that is selected should help predict the result better than this model does. 

```{r catVars_compare, echo=TRUE, warning=FALSE}

# names of columns that are categorical type and numerical type
vars <- setdiff(colnames(dTrainAll), c(outcome, 'rgroup'))
catVars <- vars[sapply(d[,vars], class) %in% c('character', 'logical',  'factor')]

mkPredC <- function(outCol, varCol, appCol) {
  pPos <- sum(outCol==pos)/length(outCol)
  naTab <- table(as.factor(outCol[is.na(varCol)]))
  pPosWna <- (naTab/sum(naTab))[pos]
  vTab <- table(as.factor(outCol), varCol)
  pPosWv <- (vTab[pos,]+1.0e-3*pPos)/(colSums(vTab)+1.0e-3)
  pred <- pPosWv[appCol]
  pred[is.na(appCol)] <- pPosWna
  pred[is.na(pred)] <- pPos
  pred
}

# now go through all the categorical variables in the `catVars` vector
# and perform the predictions. 
for (v in catVars) {
  pi <- paste('pred.', v, sep='')
  dTrain[,pi] <- mkPredC(dTrain[,outcome], dTrain[,v], dTrain[,v])
  dCal[,pi] <- mkPredC(dTrain[,outcome], dTrain[,v], dCal[,v])
  dTest[,pi] <- mkPredC(dTrain[,outcome], dTrain[,v], dTest[,v])
  
  # # it seems pred values can be greater than 1 sometimes, not sure why it happens but replacing them with median value looks better
  # dTrain <- dTrain %>%
  #   mutate(pi = ifelse(pi > 1, median(pi[pi <= 1]), pi))
  # dCal <- dCal %>%
  #   mutate(pi = ifelse(pi > 1, median(pi[pi <= 1]), pi))
  # dTest <- dTest %>%
  #   mutate(pi = ifelse(pi > 1, median(pi[pi <= 1]), pi))
}

# Define a function to compute log likelihood so that we can reuse it.
logLikelihood <- function(ypred, ytrue) {
  sum(ifelse(ytrue, log(ypred), log(1-ypred)), na.rm=T)
}

# Compute the likelihood of the Null model on the calibration
pred.Null <- sum(dTrain[,outcome]==pos)/nrow(dTrain)
cat("Proportion of outcome == True in dTrain:", pred.Null)

logNull <- logLikelihood(sum(dCal[,outcome]==pos)/nrow(dCal), dCal[,outcome]==pos)
cat("The log likelihood of the Null model is:", logNull)

selCatVars <- c()
minDrop <- -3

for (v in catVars) {
  pi <- paste('pred.', v, sep='')
  logPred <- logLikelihood(dCal[,pi], dCal[,outcome]==pos)
  cat(sprintf("%6s, log likelihood: %g\n", v, logPred))
  devDrop <- 2*(logPred - logNull)
  if (devDrop >= minDrop) {
    cat(sprintf("%6s, deviance reduction: %g\n", v, devDrop))
    selCatVars <- c(selCatVars, pi)
  }
}

devDrop <- 2*(logLikelihood(dCal[,outcome], dCal[,outcome]==pos) - logNull)
cat(sprintf("To compare, deviance reduction of the truth for %6s is: %g\n", outcome, devDrop))

```

The log likelihood and deviance reduction do not look great in any categorical/binary variables. Let is check how they perform in the ROC curve.

```{r roc_curve, echo=TRUE}
library(ROCit)

# colour_id 1-7 are: black,red,green,blue,cyan,purple,gold
plot_roc <- function(predcol, outcol, colour_id=2, overlaid=F) {
  ROCit_obj <- rocit(score=predcol, class=outcol==pos)
  par(new=overlaid)
  plot(ROCit_obj, col = c(colour_id, 1),
  legend = FALSE, YIndex = FALSE, values = FALSE)
}

print(selCatVars)

plot_roc(dCal[[selCatVars[1]]], dCal[,outcome]) #First variable is red
plot_roc(dCal[[selCatVars[2]]], dCal[,outcome], colour_id=3, overlaid=T) # Second variable is green
plot_roc(dCal[[selCatVars[3]]], dCal[,outcome], colour_id=4, overlaid=T)
```

Their AUC values are not higher than the null model. This was also confirmed in their loglikelihood and deviance reduction. 

Now we are going to use all numerical variables for prediction and see which one performs the best to predict `r outcome`.

```{r selnumVar, echo=TRUE, warning=FALSE}
numericVars <- vars[sapply(d[,vars], class) %in% c('numeric','integer')]

# get all percentage / rate variables 
rateVars <- grep("\\.rate$", numericVars, value = TRUE)

mkPredN <- function(outCol, varCol, appCol) {
  cuts <- unique(as.numeric(
    quantile(varCol, probs=seq(0, 1, 0.1), na.rm=T)))
  varC <- cut(varCol, cuts)
  appC <- cut(appCol, cuts)
  mkPredC(outCol, varC, appC)
}

# now go through all numerical variables in the `numericVars` vector and perform the predictions.
for (v in numericVars) {
  pi <- paste('pred.', v, sep='')
  dTrain[,pi] <- mkPredN(dTrain[,outcome], dTrain[,v], dTrain[,v])
  dTest[,pi] <- mkPredN(dTrain[,outcome], dTrain[,v], dTest[,v])
  dCal[,pi] <- mkPredN(dTrain[,outcome], dTrain[,v], dCal[,v])
  # # it seems pred values can be greater than 1 sometimes, not sure why it happens but replacing them with median value looks better
  # dTrain <- dTrain %>%
  #   mutate(pi = ifelse(pi > 1, median(pi[pi <= 1]), pi))
  # dCal <- dCal %>%
  #   mutate(pi = ifelse(pi > 1, median(pi[pi <= 1]), pi))
  # dTest <- dTest %>%
  #   mutate(pi = ifelse(pi > 1, median(pi[pi <= 1]), pi))
}

# selNumVars is a vector that keeps the names of the top performing numerical variables.
selNumVars <- c()
minDrop <- 0

for (v in numericVars) {
  pi <- paste('pred.', v, sep='')
  logPred <- logLikelihood(dCal[,pi], dCal[,outcome]==pos)
  
  devDrop <- 2*(logPred - logNull)
  if (devDrop >= minDrop) {
    cat(sprintf("%6s, deviance reduction: %g\n", v, devDrop))
    selNumVars <- c(selNumVars, pi)
  }
}

```

The `r selNumVars` seems to be useful in reducing deviance, especially `r selNumVars[2]`. Let us see how it performs using AUC in calibration and test dataset. Also check the result with 100 fold cross-validation.

```{r evaluate, echo=TRUE}
library(ROCR)
calcAUC <- function(ypred, ytrue) {
    perf <- performance(prediction(ypred, ytrue), 'auc')
    as.numeric(perf@y.values)
}

# selVars <- c(selCatVars, selNumVars)
selVars <- c()

## Performance of the top performing single variables on the test set:
for(v in c(selCatVars, selNumVars)) {
  pi <- v
  aucTrain <- calcAUC(dTrain[,pi], dTrain[,outcome])
  if (aucTrain > 0.52) {
    selVars <- c(selVars, pi)
    aucCal <- calcAUC(dCal[,pi], dCal[,outcome])
    print(sprintf(
      "%s: trainAUC: %4.3f; calibrationAUC: %4.3f",
      pi, aucTrain, aucCal))
  } 
}

# Run 100-fold cross validation
original_vars <- character() #sub("^pred\\.", "", selVars)

cat("Performance of the top performing single variables on the test set:")

for (v in selVars) {
  # retrieve the original variable name (character location 6 onward)
  orig_v <- substring(v, 6)
  original_vars <- c(original_vars, orig_v)
  cat(sprintf("In dTest Variable %6s: AUC = %g\n", orig_v, calcAUC(dTest[,v], dTest[,outcome]==pos)))
}

for (var in original_vars) {
  aucs <- rep(0,100)
  for (rep in 1:length(aucs)) {
    useForCalRep <- rbinom(n=nrow(dTrainAll), size=1, prob=0.1) > 0
    predRep <- mkPredC(dTrainAll[!useForCalRep, outcome],
    dTrainAll[!useForCalRep, var],
    dTrainAll[useForCalRep, var])
    aucs[rep] <- calcAUC(predRep, dTrainAll[useForCalRep, outcome])
  }
  print(sprintf("In dTrainAll %s: mean AUC: %4.3f; sd: %4.3f", var, mean(aucs), sd(aucs)))
  #if(mean(aucs) <= 0.5){
  #  original_vars <- setdiff(original_vars, var)
  #}
}

print(selVars)
print(original_vars)

```

After inspecting the best performing variables a bit more, it seems `r original_vars[1]` works very well in training and calibration dataset, but not so well in the test data. Now visualise the ROC curve for the 2 variables.

```{r roc_selNumVars, echo=TRUE}
# colour_id 1-7 are: black,red,green,blue,cyan,purple,gold
# plot_roc(dCal$pred.Year, dCal[,outcome], colour_id=2) #W red
# plot_roc(dCal$pred.Country, dCal[,outcome], colour_id=7, overlaid = T) #W gold
plot_roc(dCal$pred.total.population, dCal[,outcome], colour_id=3) # population is green
plot_roc(dCal$pred.Unsafe.sanitation, dCal[,outcome], colour_id=4, overlaid=T) #  sanitation is blue

```

It is clear that total population (green) covers a larger area. In this single variable selection part, total population seems to be the best sole variable to predict if the death rate will be higher than its worldwide median or not. Our interpretation is that the higher the population the higher death rate will be. This is probably due to the majority of the populated countries are developing countries where standard of living is less good than the developed countries, which tend to have lower population. We will take these feature variables into the multi-variate model to see if using more variables can do a better job. 


```{r double_density_cat, echo=TRUE}

# outliers <- dCal[dCal$pred.Entity > 1.0, c('Entity', 'Year', 'pred.Entity', 'Smoking.kills.more', 'Smoking', 'High.body.mass.index', 'Smoking.rate', 'High.body.mass.index.rate')]
# print(outliers)

fig1 <- ggplot(dCal) + geom_density(aes(x=pred.total.population, color=as.factor(Death.rate.high)))

fig2 <- ggplot(dCal) + geom_density(aes(x=pred.Unsafe.sanitation, color=as.factor(Death.rate.high)))

grid.arrange(fig1, fig2, ncol=1)

```


## Naive Bayers model

We will now put the `r selNumVars` in Naive Nayers model for multiple variables for prediction.

```{r run_nb_numvars, echo=TRUE, warning=FALSE}

run_nbPred <- function(outcome, selVars){

  pPos <- sum(dTrain[,outcome]==pos)/length(dTrain[,outcome])
  
  nBayes <- function(pPos, pf) {
    pNeg <- 1 - pPos
    smoothingEpsilon <- 1.0e-5
    scorePos <- log(pPos + smoothingEpsilon) +
      rowSums(log(pf/pPos + smoothingEpsilon))
    scoreNeg <- log(pNeg + smoothingEpsilon) +
      rowSums(log((1-pf)/(1-pPos) + smoothingEpsilon))
    m <- pmax(scorePos, scoreNeg)
    expScorePos <- exp(scorePos-m)
    expScoreNeg <- exp(scoreNeg-m)
    expScorePos/(expScorePos+expScoreNeg)
  }
  
  dTrain$nbpredl <- nBayes(pPos, dTrain[,selVars])
  dCal$nbpredl <- nBayes(pPos, dCal[,selVars])
  dTest$nbpredl <- nBayes(pPos, dTest[,selVars])
  
  # it seems the nbpredl has still NaN values. We will impute them with the median
  dTrain <- dTrain %>%
    mutate(nbpredl = ifelse(is.na(nbpredl), median(nbpredl, na.rm = TRUE), nbpredl))
  dCal <- dCal %>%
    mutate(nbpredl = ifelse(is.na(nbpredl), median(nbpredl, na.rm = TRUE), nbpredl))
  dTest <- dTest %>%
    mutate(nbpredl = ifelse(is.na(nbpredl), median(nbpredl, na.rm = TRUE), nbpredl))
  
  cat('Input variables are:', selVars, '\n')
  cat('The AUC value for the training set is:', calcAUC(dTrain$nbpredl, dTrain[,outcome]==pos), '\n')
  #The AUC value for the training set for uws and hapfsf is: 0.5575
  
  cat('The AUC value for the calibration set is:', calcAUC(dCal$nbpredl, dCal[,outcome]==pos), '\n')
  
  cat('The AUC value for the test set is:', calcAUC(dTest$nbpredl, dTest[,outcome]==pos))
}

#include also in NB model
selVars <- selNumVars #c(selCatVars, selNumVars[2])
run_nbPred(outcome, selVars)

```
In the single variable `r selNumVars[1]` has AUC 0.533 for training, 0.571 for calibration and 0.529 for test. `r selNumVars[2]` has 0.558 for training, 0.545 for calibration and 0.493 for test. Naive Bayers does give a slightly better result than each numeric variables alone. Let's see if adding the categorical variables helps.

```{r run_nb_catvars, echo=TRUE, warning=FALSE}
selVars <- selCatVars
run_nbPred(outcome, selVars)
```

Naive Bayers model is useful for predictions with many input variables and categorical variables with many levels. Since our categorical variables only contain 2 values true/false, they don't really improve the performance. Try it again with Country and Year as extra categorical variables.

```{r run_nb_all_catvars, echo=TRUE, warning=FALSE}
selVars <- c(selVars, 'pred.Country', 'pred.Year')
run_nbPred(outcome, selVars)
```

Note the AUC score is getting better. Now run NB model with all selected categorical and top performing numerical variables.

```{r run_nb_all_vars, echo=TRUE, warning=FALSE}
selVars <- c(selVars, selNumVars)
run_nbPred(outcome, selVars)
```

It has become 0.6 which is better than the *total.population* alone, also better than the combination of numerical variables.

## Decision tree model


```{r decision_tree, echo=TRUE}
library(rpart)
library(rpart.plot)

fV <- paste(outcome, '>0 ~ ', paste(c(selCatVars, selNumVars), collapse=' + '), sep='')
print(fV)

tmodel <- rpart(formula = fV, data=dTrain)
print(calcAUC(predict(tmodel, newdata=dTrain), dTrain[,outcome]))
## [1] 0.705922
print(calcAUC(predict(tmodel, newdata=dTest), dTest[,outcome]))
## [1] 0.6825181
print(calcAUC(predict(tmodel, newdata=dCal), dCal[,outcome]))
## [1] 0.6935539

rpart.plot(tmodel)

```


## LIME evaluation 

## Clustering