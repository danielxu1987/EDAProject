---
title: "EDA Project"
output:
  html_document: default
---

This is the R Notebook for the project from Sifeng Xu, 24525844 for unit *CITS4009 Computation Data Analysis*. It demonstrates the process and findings of EDA based on the **Countries and Death Causes** dataset. 

```{r setup, echo=FALSE}
# Load  libraries
# library(shiny)
# library(shinyWidgets)
library(ggplot2)
library(gridExtra)
library(knitr)
library(dplyr)


# load dataset
df_original <- read.csv("./Countries and death causes.csv",header = T, sep=",")

df <- df_original

dietVars <- c('Diet.low.in.whole.grains', 'Diet.low.in.fruits', 'Diet.low.in.Vegetables', 'Diet.low.in.nuts.and.seeds', 'Diet.high.in.sodium')

dietTargetVar <- c('Diet.high.in.sodium')
dietFeatureVars <- dietVars[dietVars != dietTargetVar]

count_nas <- function(data){
  count_missing <- function(data){
    sapply(data, FUN=function(col) sum(is.na(col)))
  }
  nacounts <- count_missing(data)
  hasNA = which(nacounts > 0)
  nacounts[hasNA]
}

count_null <- function(data){
  count_missing <- function(data){
    sapply(data, FUN=function(col) sum(col == ''))
  }
  nullcounts <- count_missing(data)
  hasNull = which(nullcounts > 0)
  nullcounts[hasNull]
}

count_negative <- function(data){
  count_missing <- function(data){
    sapply(data, FUN=function(col) sum(col < 0))
  }
  negcounts <- count_missing(data)
  hasNegative = which(negcounts > 0)
  negcounts[hasNegative]
}

dataUK <- df[df['Entity']=='United Kingdom',] #df[df['Entity']=='China',]
dataTAN <- df[df['Entity']=='Tanzania',]
dataNLD <- df[df['Entity']=='Netherlands',]
dataCAM <- df[df['Entity']=='Cambodia',]

```

### Basic information of the data


```{r glance, echo=TRUE}

data.frame(
  columeID = col(df)[1,],
  variables = names(df),
  class = sapply(df, typeof),
  first_values = sapply(df, function(x) paste0(head(x), collapse = ', ')),
  row.names = NULL) |>
kable()

summary(df) |> kable()

head(df) |> kable()

```


A first glance of data shows 31 columns which consists of 3 non-numerical data:

* countries' full name
* short country code
* year of measurements (1990-2019) 

the rest is numerical data showing how many deaths from each cause, each year and each country/region. Some countries are counted together such as *G20, OECD Countries, South-East Asia Region* and *Western Passific Region*. Also, some measured variables look similar such as *Outdoor.air.pollution, Household.air.pollution.from.solid.fuels* and *Air.pollution*.  


To give an impression what the dataset looks like, a snippet shows 6 countries and their deaths count from several causes in 2010.

```{r snippet, include=TRUE}
df_2010 <- df[df['Year']==2010, 
  c('Entity', 'Code', 'Outdoor.air.pollution','Child.wasting', 'Smoking', 'Drug.use')]
countries <- c('Afghanistan', 'Morocco', 'Nigeria', 'Australia', 'Belgium', 'Congo')
snippet <- subset(df_2010, Entity %in% countries)
names(snippet)[1] <- 'Country'
print(snippet)
```


### Histogram for single death cause

First check the data range by histogram. Variables that are linked to environment such as outdoor and household air quality tend to cause more deaths. This may prove that the long-term influence of poor air quality, which often sustains through a longer period, can kill many people living in that area/region.

![Outdoor air pollution](./images/hist_single_oap.JPG)
![Household air pollution from solid fuels](./images/hist_single_hap.JPG)

In contrast, causes such as *iron deficiency* and *vitamin A deficiency* lead to far less mortalities worldwide. Probably lack of iron or vitamin A alone is not too critical to the survival of most people.

![Iron deficiency](./images/hist_single_iron.JPG)
![Vitamin A deficiency](./images/hist_single_va.JPG)

Across all variables, a fact is that high value of mortality does not appear often as most values seem to stay close to the lower boundary. Note a spike near zero for all example figures.

### Boxplot single

After comparing different countries with boxplot, it is noticeable that developed countries usually have less mortalities than developing countries in causes related to poor hygiene (e.g. water quality, unsafe sanitation and no access to hand washing facility). 

![Compare countries - unsafe water source](./images/box_water.JPG)

![Compare countries - unsafe sanitation](./images/box_sani.JPG)

![Compare countries - no access to handwashing facility](./images/box_handw.JPG)

Two pairs of countries are selected here as examples between the *Uk* and *Tanzania*, as well as the *Netherlands* and *Cambodia*. The reason why they are compared to each other is because counties in each pair shares similar population according to [Worldometers](https://www.worldometers.info/world-population/population-by-country/).

### Trend comparison

Line graph shows the trend of certain causes from 1990 to 2019. Comparison between countries shows interesting facts.  Alcohol use seems to be more detrimental to people's health in recent years in both developed and developing countries selected in previous section. 

![](./images/trend_alco.JPG)

On the other hand, outdoor air pollution appears to become less harmful in western Europe than in Africa or Asia. This may have to do with the progress of industrialisation which leads to air pollution. In other words, Europe was almost at the end of it around the turn of the century, and they paid more attention to the environmental impact. While certain areas in Africa and Asia just started to industrialize themselves. That is why the air quality got worse and worse in this period there.

![](./images/trend_oap.JPG)

### Bar chart

Drug use seems to have serious impact in both rich and poor areas. In the figure below, far more people ended their life in India and the USA due to drug use than the whole African Region in 2010. European region is not much better as they ranked the third in this set of comparison worldwide.

![](./images/Bar_compare_us.JPG)

### Correlation between variables

When comparing multiple variables, it is clear that diet related death causes are  positive linear for African region. This may indicate the food security issue in this area.  On the other hand, this correlation seems less obvious for North America where the general public have much more dietary options.

![](./images/pair_diet_afr.JPG)

![](./images/pair_diet_northa.JPG)

Smooth curve and jittering points shows drug use and unsafe sex clustering when sex fatality value is low.  Once the value grows further they seemed less correlated.

![](./images/smooth_sexdrug_lim.JPG)

### Missing values and anomolies

These values are checked in all numerical variables: *NA, null, zeros* and *negative numbers*. 

```{r nas, echo=TRUE} 

numericVars <- names(df[, -c(1:3)]) # ignore Entity, Code and Year columns
df_num <- df[numericVars]

print(sum(is.na(df_num)))

print(sum(is.null(df_num)))

print(sum(df_num < 0))

print(sum(df_num == 0))
```

Currently there is no NA's in the dataset, neither are there any negative values. There are zero values, which may indicate that either no one died of that specific cause in a country in a specific year, or they were missing values. Although it is possible to treat zero's as missing values and replace them with e.g. median or mean, doing this will overwrite all the zero's which are truly reflecting zero mortality.  Therefore, in this exploration attempt we will keep zero's as-is.

When checking the first three columns it shows that *Code* are empty in 690 rows. These appear to be specific to aggregated regions e.g. OECD Countries, or regions within a country without a valid country code e.g. Scotland.

```{r rm_code, echo=TRUE}

count_empty <- function(data){
  count_missing <- function(data){
    sapply(data, FUN=function(col) sum(col == ''))
  }
  nullcounts <- count_missing(data)
  hasNull = which(nullcounts > 0)
  nullcounts[hasNull]
}

print(count_empty(df))
empty_rows <- df[df$Code == '', ]
print(empty_rows[sample(nrow(empty_rows), size = 6, replace = F), c(1:6)])
```

Since the Entity variable has no missing value and shows clear label of the region/country, we decide to drop the *Code* column for a cleaner dataset.

```{r rmcode}
df$Code <- NULL
```

```{r global, include=FALSE}
outcome <- c('sodium.over.median')
# sodium.over.median
pos <- TRUE # positive value indicator
```

### Classfication 

We would like to know more about the death cause that have large impact worldwide. Since the absolute number of mortality can vary largely depending on the population of a country, a better measurement is the percentage of each death cause, with respect to the total casualties in that row. 

We will first create a rate where it counts the percentage or weight of each specific casuality cause

```{r addCat, echo=TRUE}
df$Entity <- as.factor(df$Entity)
df$Year <- as.factor(df$Year)

df$total.mortality <- rowSums(df[numericVars])

for (col in numericVars) {
    df[paste0(col, ".rate")] <- df[col] / df['total.mortality']
}

rateVars <- grep("\\.rate$", names(df), value = TRUE)

figRateAll <- ggplot(stack(df[rateVars])) + geom_boxplot(mapping = aes(x = ind, y = values)) + labs(x ='', y='Weight in total mortality') + coord_flip() 

print(figRateAll)

df <- df |>
  mutate(
    fruit.over.median = (Diet.low.in.fruits.rate > median(Diet.low.in.fruits.rate)), # indicate if death count of low fruit consumption is over its median
    vege.over.median = (Diet.low.in.Vegetables.rate > median(Diet.low.in.Vegetables.rate)), # indicate if death count of low vegetable consumption is over its median
    grain.over.median = (Diet.low.in.whole.grains.rate > median(Diet.low.in.whole.grains.rate)),
    nutseed.over.median = (Diet.low.in.nuts.and.seeds.rate > median(Diet.low.in.nuts.and.seeds.rate)),
    sodium.over.median = (Diet.high.in.sodium.rate > median(Diet.high.in.sodium.rate)) # flag if death count of high sodium consumption is over its median
)
```

Then generate an outcome column *`r outcome``* which indicates if it contributes to the mortality more than its median. This variable holds the ground truth to check against the prediction later. Derive the same for the selected diet related variables which generates new columns, indicating if these causes contribute more over their median or not (derived binary variables). These will be the categorical variables to evaluate in the single variable selection process.

## TODO
- Take categorical variables (year, countries?)
- Make categorical vars from diet related vars (T/F, over media)
- Make death contribution rates and reserve them as numeric variables for Naive Bayers and Decision Tree modelling
- Compare the different prediction methods and scoring (AUC)


Now it is time to separate the data into training (81%), calibration(9%) and test (10%) data.

```{r get_train_cal_test, echo=TRUE}

d <- df

set.seed(729375)
d$rgroup <- runif(dim(d)[1])
dTrainAll <- subset(d, rgroup<=0.9)
dTest <- subset(d, rgroup>0.9)

# names of columns that are categorical type and numerical type
vars <- setdiff(colnames(dTrainAll), c(outcome, 'rgroup'))
catVars <- vars[sapply(df[,vars], class) %in% c('character', 'logical', 'factor')]
numericVars <- vars[sapply(df[,vars], class) %in% c('numeric','integer')]


# split dTrainAll into a training set and a validation (or calibration) set
useForCal <- rbinom(n=dim(dTrainAll)[1], size=1, prob=0.1)>0
dCal <- subset(dTrainAll, useForCal)
dTrain <- subset(dTrainAll, !useForCal)

print(dim(dTrain))

print(dim(dCal))

print(dim(dTest))

```

The next step is to iterate through the generated True/False diet related variables and see which one has the best prediction chance. Here the null model is saying the prediction will be the overall proportion of True values, regardless what input it receives. Therefore, anything that is selected should help predict the result better than this null model. 

```{r single_var_compare, echo=TRUE, warning=FALSE}
mkPredC <- function(outCol, varCol, appCol) {
  pPos <- sum(outCol==pos)/length(outCol)
  naTab <- table(as.factor(outCol[is.na(varCol)]))
  pPosWna <- (naTab/sum(naTab))[pos]
  vTab <- table(as.factor(outCol), varCol)
  pPosWv <- (vTab[pos,]+1.0e-3*pPos)/(colSums(vTab)+1.0e-3)
  pred <- pPosWv[appCol]
  pred[is.na(appCol)] <- pPosWna
  pred[is.na(pred)] <- pPos
  pred
}

# now go through all the categorical variables in the `catVars` vector
# and perform the predictions. 
for (v in catVars) {
  pi <- paste('pred.', v, sep='')
  dTrain[,pi] <- mkPredC(dTrain[,outcome], dTrain[,v], dTrain[,v])
  dCal[,pi] <- mkPredC(dTrain[,outcome], dTrain[,v], dCal[,v])
  dTest[,pi] <- mkPredC(dTrain[,outcome], dTrain[,v], dTest[,v])
}

# Define a function to compute log likelihood so that we can reuse it.
logLikelihood <- function(ypred, ytrue) {
  sum(ifelse(ytrue, log(ypred), log(1-ypred)), na.rm=T)
}

# Compute the likelihood of the Null model on the calibration
logNull <- logLikelihood(sum(dCal[,outcome]==pos)/nrow(dCal), dCal[,outcome]==pos)
cat("The log likelihood of the Null model is:", logNull)

selCatVars <- c()

for (v in catVars) {
  pi <- paste('pred.', v, sep='')
  logPred <- logLikelihood(dCal[,pi], dCal[,outcome]==pos)
  devDrop <- 2*(logPred - logNull)
  
  cat(sprintf("%6s, deviance reduction: %g\n", v, devDrop))
  selCatVars <- c(selCatVars, pi)
}

devDrop <- 2*(logLikelihood(dCal[,outcome], dCal[,outcome]==pos) - logNull)
cat(sprintf("%6s, To compare, deviance reduction of the ground truth is: %g\n", outcome, devDrop))

```

Now let's check the result of best variable with 100 fold cross-validation.



Now use all numerical variables for prediction and see which one performs the best to predict contribution of death from `r outcome`.

```{r selnumVar, echo=TRUE, warning=FALSE}
# get all percentage / rate variables 
rateVars <- grep("\\.rate$", numericVars, value = TRUE)

mkPredN <- function(outCol, varCol, appCol) {
  cuts <- unique(as.numeric(
    quantile(varCol, probs=seq(0, 1, 0.1), na.rm=T)))
  varC <- cut(varCol, cuts)
  appC <- cut(appCol, cuts)
  mkPredC(outCol, varC, appC)
}

# now go through all the diet related numerical variables in the `numericVars` vector and perform the predictions.
for (v in rateVars) {
  pi <- paste('pred.', v, sep='')
  dTrain[,pi] <- mkPredN(dTrain[,outcome], dTrain[,v], dTrain[,v])
  dTest[,pi] <- mkPredN(dTrain[,outcome], dTrain[,v], dTest[,v])
  dCal[,pi] <- mkPredN(dTrain[,outcome], dTrain[,v], dCal[,v])
}

# selNumVars is a vector that keeps the names of the top performing numerical variables.
selNumVars <- c()
minDrop <- 0

for (v in rateVars) {
  pi <- paste('pred.', v, sep='')
  logPred <- logLikelihood(dCal[,pi], dCal[,outcome]==pos)
  
  devDrop <- 2*(logPred - logNull)
  #if (devDrop >= minDrop) {
    cat(sprintf("%6s, deviance reduction: %g\n", v, devDrop))
    selNumVars <- c(selNumVars, pi)
  #}
}

```

Now that both types of variables are shortlisted, let us see how it performs using AUC in calibration and test dataset.

```{r evaluate, echo=TRUE}
library(ROCR)
calcAUC <- function(ypred, ytrue) {
  perf <- performance(prediction(ypred, ytrue), 'auc')
  as.numeric(perf@y.values)
}

selVars <- c(selCatVars, selNumVars)

## Performance of the top performing single variables on the test set:
for(v in selVars) {
  pi <- v
  aucTrain <- calcAUC(dTrain[,pi], dTrain[,outcome])
  if (aucTrain >= 0.51) {
    aucCal <- calcAUC(dCal[,pi], dCal[,outcome])
    print(sprintf(
      "%s: trainAUC: %4.3f; calibrationAUC: %4.3f",
      pi, aucTrain, aucCal))
  }
}

# Run 100-fold cross validation
vars <- catVars

for (var in vars) {
  aucs <- rep(0,100)
  for (rep in 1:length(aucs)) {
    useForCalRep <- rbinom(n=nrow(dTrainAll), size=1, prob=0.1) > 0
    predRep <- mkPredC(dTrainAll[!useForCalRep, outcome],
    dTrainAll[!useForCalRep, var],
    dTrainAll[useForCalRep, var])
    aucs[rep] <- calcAUC(predRep, dTrainAll[useForCalRep, outcome])
  }
  print(sprintf("%s: mean: %4.3f; sd: %4.3f", var, mean(aucs), sd(aucs)))
}

```

In the result, all diet related variables perform poorly as they only have 50% chance of getting True or False. That is not better than a random guessing. In the meantime, three variables seem to be most helpful, although their AUC values are not very high (0.8+) either: 

- pred.**Unsafe.sanitation**.rate: trainAUC: 0.551; calibrationAUC: 0.530
- pred.**Diet.low.in.nuts.and.seeds**.rate: trainAUC: 0.515; calibrationAUC: 0.549
- pred.**Non.exclusive.breastfeeding**.rate: trainAUC: 0.506; calibrationAUC: 0.514

```{r double_density_cat, echo=TRUE}

fig1 <- ggplot(dCal) + geom_density(aes(x=pred.fruit.over.median, color=as.factor(sodium.over.median)))
print(fig1)
```

